{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["from tensorflow import keras\n","import numpy as np\n","fashion_mnist = keras.datasets.fashion_mnist\n","(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n","train_images = train_images / 255.0\n","test_images = test_images / 255.0\n","idx = np.arange(train_images.shape[0])\n","np.random.shuffle(idx)\n","train_images=train_images[idx]\n","train_labels=train_labels[idx]\n","validation_images=train_images[:6000]\n","validation_labels=train_labels[:6000]\n","train_images=train_images[6000:]\n","train_labels=train_labels[6000:]"]},{"cell_type":"markdown","metadata":{},"source":["# **Q4 Q5 Q6**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Helper:\n","  def sigmoid(self,layer,d=False):\n","      if(d==True):\n","          return self.sigmoid(layer)*(1-self.sigmoid(layer))\n","      return 1.0/(1.0+np.exp(-1.0*layer))\n","\n","  def identity(self,layer,d=False):\n","      if(d==True):\n","          return np.ones(layer.shape)\n","      return layer\n","\n","  def softmax(self,layer,d=False):\n","      if(d==True):\n","          s=self.softmax(layer,False);\n","          return s*(1-s)\n","      newlayer=(layer-np.max(layer))\n","      return np.exp(newlayer)/np.sum(np.exp(newlayer))\n","\n","  def cross_entropy(self,true_output,output):\n","      return -1.0*np.sum(true_output * np.log(output+1e-9))\n","  \n","  def mean_square_error(self,true_output,output):\n","      return np.sum((true_output-output)**2)\n","\n","  def tanh(self,layer,d=False):\n","      if(d==True):\n","          return 1-np.tanh(layer)**2\n","      return np.tanh(layer)\n","\n","  def relu(self,layer,d=False):\n","      if(d==True):\n","        return 1. * (layer > 0)\n","      return layer * (layer > 0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class sgd:\n","  def optimize(self,num_hidden_layers,weights,bias,learning_rate,d_weights,d_bias):\n","    for i in range(1,num_hidden_layers+2):\n","      weights[i]=weights[i]-learning_rate*d_weights[i]\n","      bias[i]=bias[i]-learning_rate*d_bias[i]\n","    return weights,bias\n","\n","class momentum:\n","  def __init__(self,neurons_list):\n","    self.update_w={}\n","    self.update_b={}\n","    for i in range(len(neurons_list)-1):\n","      self.update_w[i+1]=np.zeros((neurons_list[i+1],neurons_list[i]))\n","      self.update_b[i+1]=np.zeros((neurons_list[i+1],1))\n","  def optimize(self,num_hidden_layers,weights,bias,learning_rate,d_weights,d_bias,beta):\n","    for i in range(1,num_hidden_layers+2):\n","        self.update_w[i]=beta*self.update_w[i]+d_weights[i]\n","        self.update_b[i]=beta*self.update_b[i]+d_bias[i]\n","\n","    for i in range(1,num_hidden_layers+2):\n","      weights[i]=weights[i]-learning_rate*self.update_w[i]\n","      bias[i]=bias[i]-learning_rate*self.update_b[i]\n","\n","    return weights,bias\n","\n","class nag:\n","  def __init__(self,neurons_list,weights,bias):\n","    self.update_w={}\n","    self.update_b={}\n","    self.weights_use={}\n","    self.bias_use={}\n","    for i in range(len(neurons_list)-1):\n","      self.update_w[i+1]=np.zeros((neurons_list[i+1],neurons_list[i]))\n","      self.update_b[i+1]=np.zeros((neurons_list[i+1],1))\n","    self.bias_use=bias.copy()\n","    self.weights_use=weights.copy()\n","  \n","  def optimize(self,num_hidden_layers,weights,bias,learning_rate,d_weights,d_bias,beta):\n","    for i in range(1,num_hidden_layers+2):\n","        self.update_w[i]=beta*self.update_w[i]+d_weights[i]\n","        self.update_b[i]=beta*self.update_b[i]+d_bias[i]\n","\n","    for i in range(1,num_hidden_layers+2):\n","      self.weights_use[i]=self.weights_use[i]-learning_rate*self.update_w[i]\n","      self.bias_use[i]=self.bias_use[i]-learning_rate*self.update_b[i]\n","\n","    for i in range(1,num_hidden_layers+2):\n","      weights[i]=self.weights_use[i]-beta*self.update_w[i]\n","      bias[i]=self.bias_use[i]-beta*self.update_b[i] \n","\n","    return weights,bias,self.weights_use,self.bias_use\n","\n","class rmsprop:\n","  def __init__(self,neurons_list):\n","    self.update_w={}\n","    self.update_b={}\n","    self.epsilon=1e-6\n","    for i in range(len(neurons_list)-1):\n","      self.update_w[i+1]=np.zeros((neurons_list[i+1],neurons_list[i]))\n","      self.update_b[i+1]=np.zeros((neurons_list[i+1],1))\n","\n","  def optimize(self,num_hidden_layers,weights,bias,learning_rate,d_weights,d_bias,beta):\n","    for i in range(1,num_hidden_layers+2):\n","        self.update_w[i]=beta*self.update_w[i]+(1-beta)*((d_weights[i])**2)\n","        self.update_b[i]=beta*self.update_b[i]+(1-beta)*((d_bias[i])**2)\n","\n","    for i in range(1,num_hidden_layers+2):\n","      weights[i]=weights[i]-learning_rate*d_weights[i]/(np.sqrt(self.update_w[i])+self.epsilon)\n","      bias[i]=bias[i]-learning_rate*d_bias[i]/(np.sqrt(self.update_b[i])+self.epsilon)\n","      \n","    return weights,bias\n","\n","\n","\n","class adam:\n","  def __init__(self,neurons_list):\n","    self.update_w={}\n","    self.update_b={}\n","    self.update_what={}\n","    self.update_bhat={}\n","    self.momentum_w={}\n","    self.momentum_b={}\n","    self.momentum_what={}\n","    self.momentum_bhat={}\n","    self.epsilon=1e-6\n","    for i in range(len(neurons_list)-1):\n","      self.update_w[i+1]=np.zeros((neurons_list[i+1],neurons_list[i]))\n","      self.update_b[i+1]=np.zeros((neurons_list[i+1],1))\n","      self.momentum_w[i+1]=np.zeros((neurons_list[i+1],neurons_list[i]))\n","      self.momentum_b[i+1]=np.zeros((neurons_list[i+1],1))\n","  def optimize(self,num_hidden_layers,weights,bias,learning_rate,d_weights,d_bias,beta1,beta2,t):\n","    for i in range(1,num_hidden_layers+2):\n","        self.momentum_w[i]=beta1*self.momentum_w[i]+(1-beta1)*((d_weights[i]))\n","        self.momentum_what[i]=self.momentum_w[i]/(1-beta1**t)\n","\n","        self.momentum_b[i]=beta1*self.momentum_b[i]+(1-beta1)*((d_bias[i]))\n","        self.momentum_bhat[i]=self.momentum_b[i]/(1-beta1**t)\n","\n","        self.update_w[i]=beta2*self.update_w[i]+(1-beta2)*((d_weights[i])**2)\n","        self.update_what[i]=self.update_w[i]/(1-beta2**t)\n","\n","        self.update_b[i]=beta2*self.update_b[i]+(1-beta2)*((d_bias[i])**2)\n","        self.update_bhat[i]=self.update_b[i]/(1-beta2**t)\n","  \n","\n","    for i in range(1,num_hidden_layers+2):\n","      weights[i]=weights[i]-learning_rate*self.momentum_what[i]/(np.sqrt(self.update_what[i])+self.epsilon)\n","      bias[i]=bias[i]-learning_rate*self.momentum_bhat[i]/(np.sqrt(self.update_bhat[i])+self.epsilon)\n","      \n","    return weights,bias\n","    \n","    \n","\n","class nadam:\n","  def __init__(self,neurons_list):\n","    self.update_w={}\n","    self.update_b={}\n","    self.update_what={}\n","    self.update_bhat={}\n","    self.momentum_w={}\n","    self.momentum_b={}\n","    self.momentum_what={}\n","    self.momentum_bhat={}\n","    self.epsilon=1e-6\n","    for i in range(len(neurons_list)-1):\n","      self.update_w[i+1]=np.zeros((neurons_list[i+1],neurons_list[i]))\n","      self.update_b[i+1]=np.zeros((neurons_list[i+1],1))\n","      self.momentum_w[i+1]=np.zeros((neurons_list[i+1],neurons_list[i]))\n","      self.momentum_b[i+1]=np.zeros((neurons_list[i+1],1))\n","  def optimize(self,num_hidden_layers,weights,bias,learning_rate,d_weights,d_bias,beta1,beta2,t):\n","    for i in range(1,num_hidden_layers+2):\n","        self.momentum_w[i]=beta1*self.momentum_w[i]+(1-beta1)*((d_weights[i]))\n","        self.momentum_what[i]=self.momentum_w[i]/(1-beta1**t)\n","\n","        self.momentum_b[i]=beta1*self.momentum_b[i]+(1-beta1)*((d_bias[i]))\n","        self.momentum_bhat[i]=self.momentum_b[i]/(1-beta1**t)\n","\n","        self.update_w[i]=beta2*self.update_w[i]+(1-beta2)*((d_weights[i])**2)\n","        self.update_what[i]=self.update_w[i]/(1-beta2**t)\n","\n","        self.update_b[i]=beta2*self.update_b[i]+(1-beta2)*((d_bias[i])**2)\n","        self.update_bhat[i]=self.update_b[i]/(1-beta2**t)\n","  \n","\n","    for i in range(1,num_hidden_layers+2):\n","      weights[i]=weights[i]-learning_rate/(np.sqrt(self.update_what[i])+self.epsilon)*(beta1*self.momentum_what[i]+(1-beta1)*d_weights[i]/(1-beta1**t))\n","      bias[i]=bias[i]-learning_rate/(np.sqrt(self.update_bhat[i])+self.epsilon)*(beta1*self.momentum_bhat[i]+(1-beta1)*d_bias[i]/(1-beta1**t))\n","      \n","    return weights,bias\n","  \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class NeuralNetwork:\n","  actobj=Helper()\n","  def __init__(self,num_hidden_layers=3,num_neurons_in_hidden_layer=64,learning_rate=0.001,epoch=10,batch=64,activation=\"tanh\",beta=0.9,optimizer=\"rmsprop\",init_strat=\"xavier\",weight_decay=0,loss_type=\"cross_entropy\"):\n","    self.num_hidden_layers=num_hidden_layers\n","    self.num_neurons_in_hidden_layer=num_neurons_in_hidden_layer\n","    self.learning_rate=learning_rate\n","    self.epoch=epoch\n","    self.batch=batch\n","    self.pre_activation_layer={}\n","    self.activation_layer={}\n","    self.weights={}\n","    self.bias={}\n","    self.d_pre_activation_layer={}\n","    self.d_activation_layer={}\n","    self.d_weights={}\n","    self.d_bias={}\n","    self.beta=beta\n","    self.beta1=0.999\n","    self.optimizer=optimizer\n","    self.init_strat=init_strat\n","    self.weight_decay=weight_decay\n","    self.loss_type=loss_type\n","    # self.activation_fn=activation\n","    if(activation==\"sigmoid\"):self.activation_fn=NeuralNetwork.actobj.sigmoid\n","    if(activation==\"tanh\"):self.activation_fn=NeuralNetwork.actobj.tanh\n","    if(activation==\"relu\"):self.activation_fn=NeuralNetwork.actobj.relu\n","    if(loss_type==\"cross_entropy\"): self.loss_fn=NeuralNetwork.actobj.cross_entropy\n","    if(loss_type==\"mean_square_error\"): self.loss_fn=NeuralNetwork.actobj.mean_square_error\n","\n","  def weight_initialization(self):\n","    self.neurons_list=[self.input_size]\n","\n","    for i in range(self.num_hidden_layers):\n","      self.neurons_list.append(self.num_neurons_in_hidden_layer)\n","    self.neurons_list.append(self.output_size)\n","\n","    for i in range(len(self.neurons_list)-1):\n","      if(self.init_strat==\"random\"):\n","        self.weights[i+1]=np.random.default_rng().uniform(-1,1,(self.neurons_list[i+1],self.neurons_list[i]))\n","        self.bias[i+1]=np.random.default_rng().uniform(-1,1,(self.neurons_list[i+1],1))\n","      elif(self.init_strat==\"xavier\"):\n","        self.weights[i+1]=np.random.randn(self.neurons_list[i+1],self.neurons_list[i])*np.sqrt(2/(self.neurons_list[i+1]+self.neurons_list[i]))\n","        self.bias[i+1]=np.random.randn(self.neurons_list[i+1],1)*np.sqrt(2/(self.neurons_list[i+1]+1))\n","  \n","  def forward_propogation(self,x):\n","    self.activation_layer[0]=x\n","    for i in range(1,self.num_hidden_layers+1):\n","      self.pre_activation_layer[i]=self.bias[i]+np.matmul(self.weights[i],self.activation_layer[i-1])\n","      self.activation_layer[i]=self.activation_fn(self.pre_activation_layer[i])\n","    self.pre_activation_layer[self.num_hidden_layers+1]=self.bias[self.num_hidden_layers+1]+np.matmul(self.weights[self.num_hidden_layers+1],self.activation_layer[self.num_hidden_layers])\n","    self.activation_layer[self.num_hidden_layers+1]=NeuralNetwork.actobj.softmax(self.pre_activation_layer[self.num_hidden_layers+1])    \n","    return self.activation_layer[self.num_hidden_layers+1]\n","\n","  def back_propogation(self,true_output,output):\n","    d_pre_activation_layer={}\n","    d_weights={}\n","    d_activation_layer={}\n","    d_bias={}\n","    if(self.loss_type==\"cross_entropy\"):\n","      d_pre_activation_layer[self.num_hidden_layers+1]=-1*(true_output-output)\n","    elif(self.loss_type==\"mean_square_error\"):\n","      d_pre_activation_layer[self.num_hidden_layers+1]=-2*(true_output-output)*(output*(1-output))\n","    for i in range(self.num_hidden_layers+1,0,-1):\n","      d_weights[i]=np.outer(d_pre_activation_layer[i],self.activation_layer[i-1].T)\n","      d_bias[i]=d_pre_activation_layer[i]\n","      if(i==1):break\n","      d_activation_layer[i-1]=np.matmul(self.weights[i].T,d_pre_activation_layer[i])\n","      d_pre_activation_layer[i-1]=np.multiply(d_activation_layer[i-1],self.activation_fn(self.pre_activation_layer[i-1],True))#element multiplication\n","\n","    return d_weights,d_bias,self.loss_fn(true_output,output)\n","    \n","\n","  def flush_gradients(self):\n","    for i in range(len(self.neurons_list)-1):\n","      self.d_weights[i+1]=np.zeros((self.neurons_list[i+1],self.neurons_list[i]))\n","      self.d_bias[i+1]=np.zeros((self.neurons_list[i+1],1))\n","\n","  def accuracy(self,images,labels):\n","    count=0\n","    loss=0\n","    for i in range(images.shape[0]):\n","      output=self.forward_propogation(images[i].reshape((-1,1)))\n","      true_output=np.zeros((self.output_size,1))\n","      true_output[labels[i]]=1\n","      if(labels[i]==np.argmax(output)):\n","        count+=1\n","      loss+=self.loss_fn(true_output,output)\n","    \n","    for i in range(1,self.num_hidden_layers+2):\n","      loss+=(self.weight_decay/2)*np.sum(np.square(self.weights[i]))\n","\n","    return count*1.0/images.shape[0], loss/images.shape[0]\n","    \n","\n","  def run(self,train_images,train_labels,validation_images,validation_labels):\n","    self.input_size=train_images.shape[1]*train_images.shape[2]\n","    self.output_size=10\n","    self.weight_initialization()\n","\n","    if self.optimizer==\"sgd\":\n","      opt=sgd()\n","    elif self.optimizer==\"momentum\":\n","      opt=momentum(self.neurons_list)\n","    elif self.optimizer==\"rmsprop\":\n","      opt=rmsprop(self.neurons_list)\n","    elif self.optimizer==\"nag\":\n","      opt=nag(self.neurons_list,self.weights,self.bias)\n","    elif self.optimizer==\"adam\":\n","      opt=adam(self.neurons_list)\n","    elif self.optimizer==\"nadam\":\n","      opt=nadam(self.neurons_list)\n","\n","    t=0\n","    for epoch_no in range(self.epoch):\n","      val_loss,tra_loss,tra_acc,val_acc=0,0,0,0\n","      loss=0\n","      batch_count=0\n","      self.flush_gradients()\n","      for i in range(train_images.shape[0]):\n","        batch_count+=1\n","        true_output=np.zeros((self.output_size,1))\n","        true_output[train_labels[i]]=1\n","        output=self.forward_propogation(train_images[i].reshape((-1,1)))\n","        d_weights,d_bias,loss=self.back_propogation(true_output,output)\n","        for j in range(1,self.num_hidden_layers+2):\n","            self.d_weights[j]=self.d_weights[j]+d_weights[j]\n","            self.d_bias[j]=self.d_bias[j]+d_bias[j]\n","\n","\n","        if(batch_count==self.batch):\n","          for j in range(1,self.num_hidden_layers+2):\n","            self.d_weights[j]=self.d_weights[j]+self.weight_decay*self.weights[j]\n","\n","          self.d_weights = {k: v / self.batch for k, v in self.d_weights.items()}\n","          self.d_bias = {k: v / self.batch for k, v in self.d_bias.items()}\n","          batch_count=0\n","          if self.optimizer==\"sgd\":\n","            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias)\n","          elif self.optimizer==\"momentum\":\n","            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta)\n","          elif self.optimizer==\"rmsprop\":\n","            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta)\n","          elif self.optimizer==\"adam\":\n","            t+=1\n","            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta,self.beta1,t)\n","          elif self.optimizer==\"nadam\":\n","            t+=1\n","            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta,self.beta1,t)\n","          elif self.optimizer==\"nag\":\n","            self.weights,self.bias,self.weights_use,self.bias_use=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta)\n","\n","          self.flush_gradients()\n","\n","      if(batch_count>0):#remaining\n","          for j in range(1,self.num_hidden_layers+2):\n","            self.d_weights[j]=self.d_weights[j]+self.weight_decay*self.weights[j]\n","\n","          self.d_weights = {k: v / batch_count for k, v in self.d_weights.items()}\n","          self.d_bias = {k: v / batch_count for k, v in self.d_bias.items()}\n","          batch_count=0\n","          if self.optimizer==\"sgd\":\n","            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias)\n","          elif self.optimizer==\"momentum\":\n","            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta)\n","          elif self.optimizer==\"rmsprop\":\n","            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta)\n","          elif self.optimizer==\"adam\":\n","            t+=1\n","            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta,self.beta1,t)\n","          elif self.optimizer==\"nadam\":\n","            t+=1\n","            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta,self.beta1,t)\n","          elif self.optimizer==\"nag\":\n","            self.weights,self.bias,self.weights_use,self.bias_use=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta)\n","\n","          self.flush_gradients()     \n","\n","      val_acc,val_loss=self.accuracy(validation_images,validation_labels)\n","      tra_acc,tra_loss=self.accuracy(train_images,train_labels) \n","      print(epoch_no+1,'Training loss',tra_loss,'Val loss',val_loss,'Training Accuracy',tra_acc,'Val Accuracy',val_acc)\n","      wandb.log({\"Training loss\":tra_loss,'Val loss':val_loss,'Training Accuracy':tra_acc,'Val Accuracy':val_acc})\n","    if self.optimizer==\"nag\":\n","      self.weights=self.weights_use #for nag\n","      self.bias=self.bias_use #for nag\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install wandb\n","import wandb\n","wandb.login(key=\"279a68e0fd5d16d5893ca46bdc25076ad1f3be50\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train():\n","  config_defaults={\n","    'num_hidden_layers':3,\n","    'num_neurons_in_hidden_layer':128,\n","    'learning_rate':0.001,\n","    'epoch':10,\n","    'batch':32,\n","    'activation':\"sigmoid\",\n","    'beta':0.9,\n","    'optimizer':'adam',\n","    'init_strat':\"xavier\",\n","    'weight_decay':0,\n","    'loss_type':\"cross_entropy\"\n","  }\n","  wandb.init(project=\"DLCS6910\",entity=\"cs22m072\",config=config_defaults)\n","  config=wandb.config\n","  num_hidden_layers=config.num_hidden_layers\n","  num_neurons_in_hidden_layer=config.num_neurons_in_hidden_layer\n","  learning_rate=config.learning_rate\n","  epoch=config.epoch\n","  batch=config.batch\n","  activation=config.activation\n","  beta=config.beta\n","  optimizer=config.optimizer\n","  init_strat=config.init_strat\n","  weight_decay=config.weight_decay\n","  loss_type=config.loss_type\n","  wandb.run.name=\"hl_\"+str(num_hidden_layers)+\"_nn_\"+str(num_neurons_in_hidden_layer)+\"_lr_\"+str(learning_rate)+\"_ep_\"+str(epoch)+\"_opt_\"+optimizer+\"_bs_\"+str(batch)+\"_act_\"+activation+\"_b_\"+str(beta)+\"_init_\"+init_strat+\"_l2_\"+str(weight_decay)\n","  nn=NeuralNetwork(num_hidden_layers,num_neurons_in_hidden_layer,learning_rate,epoch,batch,activation,beta,optimizer,init_strat,weight_decay,loss_type)\n","  nn.run(train_images,train_labels,validation_images,validation_labels)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sweep_config={\n","    'method':'random',\n","    'metric':{\n","      'name':'Val Accuracy',\n","      'goal':'maximize'\n","    },\n","    'parameters':{\n","      'num_hidden_layers':{\n","          'values':[3,4,5]\n","      },\n","      'num_neurons_in_hidden_layer':{\n","          'values':[36,64,128]\n","      },\n","      'learning_rate':{\n","          'values':[0.1,0.01,0.001]\n","      },\n","      'epoch':{\n","          'values':[5,10]\n","      },\n","      'batch':{\n","          'values':[16,32,64]\n","      },\n","      'activation':{\n","          'values':[\"sigmoid\",\"relu\",\"tanh\"]\n","      },\n","      'beta':{\n","          'values':[0.9]\n","      },\n","      'optimizer':{\n","          'values':[\"sgd\",\"adam\",\"nadam\",\"rmsprop\",\"nag\",\"momentum\"]\n","      },\n","      'init_strat':{\n","          'values':[\"random\",\"xavier\"]\n","      },\n","      'weight_decay':{\n","          'values':[0,0.0005,0.5]\n","      },\n","      'loss_type':{\n","          'values':[\"cross_entropy\"]\n","      }\n","    }\n","\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sweep_id=wandb.sweep(sweep_config,project=\"DLCS6910\",entity=\"cs22m072\")\n","# wandb.init(project=\"DLCS6910\",entity=\"cs22m072\")\n","\n","wandb.agent(sweep_id,train,count=2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
