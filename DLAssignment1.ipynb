{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLXNG2-h-tuM"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login(key=\"279a68e0fd5d16d5893ca46bdc25076ad1f3be50\")\n",
        "wandb.init(project=\"DLCS6910\",entity=\"cs22m072\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "idx = np.arange(train_images.shape[0])\n",
        "np.random.shuffle(idx)\n",
        "train_images=train_images[idx]\n",
        "train_labels=train_labels[idx]\n",
        "validation_images=train_images[:6000]\n",
        "validation_labels=train_labels[:6000]\n",
        "train_images=train_images[6000:]\n",
        "train_labels=train_labels[6000:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToVlKdcEYb4l",
        "outputId": "33a124a9-7089-4a73-e1fa-021b6831f9ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q1**"
      ],
      "metadata": {
        "id": "lkDptiEJYmd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "\n",
        "cur_label=0\n",
        "imgs=[]\n",
        "lbls=[]\n",
        "for i in range(len(train_images)):\n",
        "  if(cur_label==train_labels[i]):\n",
        "    imgs.append(train_images[i])\n",
        "    lbls.append(labels[cur_label])\n",
        "    cur_label+=1\n",
        "    if(cur_label==10):break\n",
        "\n",
        "wandb.log({\"Image\": [wandb.Image(imgs[i], caption=lbls[i]) for i in range(10)]})"
      ],
      "metadata": {
        "id": "kAy_mcINYg6z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q2 Q3**"
      ],
      "metadata": {
        "id": "790JH9nYZfTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Helper:\n",
        "  def sigmoid(self,layer,d=False):\n",
        "      if(d==True):\n",
        "          return self.sigmoid(layer)*(1-self.sigmoid(layer))\n",
        "      return 1.0/(1.0+np.exp(-1.0*layer))\n",
        "\n",
        "  def identity(self,layer,d=False):\n",
        "      if(d==True):\n",
        "          return np.ones(layer.shape)\n",
        "      return layer\n",
        "\n",
        "  def softmax(self,layer,d=False):\n",
        "      if(d==True):\n",
        "          s=self.softmax(layer,False);\n",
        "          return s*(1-s)\n",
        "      newlayer=(layer-np.max(layer))\n",
        "      return np.exp(newlayer)/np.sum(np.exp(newlayer))\n",
        "\n",
        "  def cross_entropy(self,true_output,output):\n",
        "      return -1.0*np.sum(true_output * np.log(output+1e-9))\n",
        "  \n",
        "  def mean_square_error(self,true_output,output):\n",
        "      return np.sum((true_output-output)**2)\n",
        "\n",
        "  def tanh(self,layer,d=False):\n",
        "      if(d==True):\n",
        "          return 1-np.tanh(layer)**2\n",
        "      return np.tanh(layer)\n",
        "\n",
        "  def relu(self,layer,d=False):\n",
        "      if(d==True):\n",
        "        return 1. * (layer > 0)\n",
        "      return layer * (layer > 0)\n"
      ],
      "metadata": {
        "id": "Jrhne7dJYu_v"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class sgd:\n",
        "  def optimize(self,num_hidden_layers,weights,bias,learning_rate,d_weights,d_bias):\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "      weights[i]=weights[i]-learning_rate*d_weights[i]\n",
        "      bias[i]=bias[i]-learning_rate*d_bias[i]\n",
        "    return weights,bias\n",
        "\n",
        "class momentum:\n",
        "  def __init__(self,neurons_list):\n",
        "    self.update_w={}\n",
        "    self.update_b={}\n",
        "    for i in range(len(neurons_list)-1):\n",
        "      self.update_w[i+1]=np.zeros((neurons_list[i+1],neurons_list[i]))\n",
        "      self.update_b[i+1]=np.zeros((neurons_list[i+1],1))\n",
        "  def optimize(self,num_hidden_layers,weights,bias,learning_rate,d_weights,d_bias,beta):\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "        self.update_w[i]=beta*self.update_w[i]+d_weights[i]\n",
        "        self.update_b[i]=beta*self.update_b[i]+d_bias[i]\n",
        "\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "      weights[i]=weights[i]-learning_rate*self.update_w[i]\n",
        "      bias[i]=bias[i]-learning_rate*self.update_b[i]\n",
        "\n",
        "    return weights,bias\n",
        "\n",
        "class nag:\n",
        "  def __init__(self,neurons_list,weights,bias):\n",
        "    self.update_w={}\n",
        "    self.update_b={}\n",
        "    self.weights_use={}\n",
        "    self.bias_use={}\n",
        "    for i in range(len(neurons_list)-1):\n",
        "      self.update_w[i+1]=np.zeros((neurons_list[i+1],neurons_list[i]))\n",
        "      self.update_b[i+1]=np.zeros((neurons_list[i+1],1))\n",
        "    self.bias_use=bias.copy()\n",
        "    self.weights_use=weights.copy()\n",
        "  \n",
        "  def optimize(self,num_hidden_layers,weights,bias,learning_rate,d_weights,d_bias,beta):\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "        self.update_w[i]=beta*self.update_w[i]+d_weights[i]\n",
        "        self.update_b[i]=beta*self.update_b[i]+d_bias[i]\n",
        "\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "      self.weights_use[i]=self.weights_use[i]-learning_rate*self.update_w[i]\n",
        "      self.bias_use[i]=self.bias_use[i]-learning_rate*self.update_b[i]\n",
        "\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "      weights[i]=self.weights_use[i]-beta*self.update_w[i]\n",
        "      bias[i]=self.bias_use[i]-beta*self.update_b[i] \n",
        "\n",
        "    return weights,bias,self.weights_use,self.bias_use\n",
        "\n",
        "class rmsprop:\n",
        "  def __init__(self,neurons_list):\n",
        "    self.update_w={}\n",
        "    self.update_b={}\n",
        "    self.epsilon=1e-6\n",
        "    for i in range(len(neurons_list)-1):\n",
        "      self.update_w[i+1]=np.zeros((neurons_list[i+1],neurons_list[i]))\n",
        "      self.update_b[i+1]=np.zeros((neurons_list[i+1],1))\n",
        "\n",
        "  def optimize(self,num_hidden_layers,weights,bias,learning_rate,d_weights,d_bias,beta):\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "        self.update_w[i]=beta*self.update_w[i]+(1-beta)*((d_weights[i])**2)\n",
        "        self.update_b[i]=beta*self.update_b[i]+(1-beta)*((d_bias[i])**2)\n",
        "\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "      weights[i]=weights[i]-learning_rate*d_weights[i]/(np.sqrt(self.update_w[i])+self.epsilon)\n",
        "      bias[i]=bias[i]-learning_rate*d_bias[i]/(np.sqrt(self.update_b[i])+self.epsilon)\n",
        "      \n",
        "    return weights,bias\n",
        "\n",
        "\n",
        "\n",
        "class adam:\n",
        "  def __init__(self,neurons_list):\n",
        "    self.update_w={}\n",
        "    self.update_b={}\n",
        "    self.update_what={}\n",
        "    self.update_bhat={}\n",
        "    self.momentum_w={}\n",
        "    self.momentum_b={}\n",
        "    self.momentum_what={}\n",
        "    self.momentum_bhat={}\n",
        "    self.epsilon=1e-6\n",
        "    for i in range(len(neurons_list)-1):\n",
        "      self.update_w[i+1]=np.zeros((neurons_list[i+1],neurons_list[i]))\n",
        "      self.update_b[i+1]=np.zeros((neurons_list[i+1],1))\n",
        "      self.momentum_w[i+1]=np.zeros((neurons_list[i+1],neurons_list[i]))\n",
        "      self.momentum_b[i+1]=np.zeros((neurons_list[i+1],1))\n",
        "  def optimize(self,num_hidden_layers,weights,bias,learning_rate,d_weights,d_bias,beta1,beta2,t):\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "        self.momentum_w[i]=beta1*self.momentum_w[i]+(1-beta1)*((d_weights[i]))\n",
        "        self.momentum_what[i]=self.momentum_w[i]/(1-beta1**t)\n",
        "\n",
        "        self.momentum_b[i]=beta1*self.momentum_b[i]+(1-beta1)*((d_bias[i]))\n",
        "        self.momentum_bhat[i]=self.momentum_b[i]/(1-beta1**t)\n",
        "\n",
        "        self.update_w[i]=beta2*self.update_w[i]+(1-beta2)*((d_weights[i])**2)\n",
        "        self.update_what[i]=self.update_w[i]/(1-beta2**t)\n",
        "\n",
        "        self.update_b[i]=beta2*self.update_b[i]+(1-beta2)*((d_bias[i])**2)\n",
        "        self.update_bhat[i]=self.update_b[i]/(1-beta2**t)\n",
        "  \n",
        "\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "      weights[i]=weights[i]-learning_rate*self.momentum_what[i]/(np.sqrt(self.update_what[i])+self.epsilon)\n",
        "      bias[i]=bias[i]-learning_rate*self.momentum_bhat[i]/(np.sqrt(self.update_bhat[i])+self.epsilon)\n",
        "      \n",
        "    return weights,bias\n",
        "    \n",
        "    \n",
        "\n",
        "class nadam:\n",
        "  def __init__(self,neurons_list):\n",
        "    self.update_w={}\n",
        "    self.update_b={}\n",
        "    self.update_what={}\n",
        "    self.update_bhat={}\n",
        "    self.momentum_w={}\n",
        "    self.momentum_b={}\n",
        "    self.momentum_what={}\n",
        "    self.momentum_bhat={}\n",
        "    self.epsilon=1e-6\n",
        "    for i in range(len(neurons_list)-1):\n",
        "      self.update_w[i+1]=np.zeros((neurons_list[i+1],neurons_list[i]))\n",
        "      self.update_b[i+1]=np.zeros((neurons_list[i+1],1))\n",
        "      self.momentum_w[i+1]=np.zeros((neurons_list[i+1],neurons_list[i]))\n",
        "      self.momentum_b[i+1]=np.zeros((neurons_list[i+1],1))\n",
        "  def optimize(self,num_hidden_layers,weights,bias,learning_rate,d_weights,d_bias,beta1,beta2,t):\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "        self.momentum_w[i]=beta1*self.momentum_w[i]+(1-beta1)*((d_weights[i]))\n",
        "        self.momentum_what[i]=self.momentum_w[i]/(1-beta1**t)\n",
        "\n",
        "        self.momentum_b[i]=beta1*self.momentum_b[i]+(1-beta1)*((d_bias[i]))\n",
        "        self.momentum_bhat[i]=self.momentum_b[i]/(1-beta1**t)\n",
        "\n",
        "        self.update_w[i]=beta2*self.update_w[i]+(1-beta2)*((d_weights[i])**2)\n",
        "        self.update_what[i]=self.update_w[i]/(1-beta2**t)\n",
        "\n",
        "        self.update_b[i]=beta2*self.update_b[i]+(1-beta2)*((d_bias[i])**2)\n",
        "        self.update_bhat[i]=self.update_b[i]/(1-beta2**t)\n",
        "  \n",
        "\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "      weights[i]=weights[i]-learning_rate/(np.sqrt(self.update_what[i])+self.epsilon)*(beta1*self.momentum_what[i]+(1-beta1)*d_weights[i]/(1-beta1**t))\n",
        "      bias[i]=bias[i]-learning_rate/(np.sqrt(self.update_bhat[i])+self.epsilon)*(beta1*self.momentum_bhat[i]+(1-beta1)*d_bias[i]/(1-beta1**t))\n",
        "      \n",
        "    return weights,bias\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OA9vZ4nDZRZL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "  actobj=Helper()\n",
        "  def __init__(self,num_hidden_layers=3,num_neurons_in_hidden_layer=64,learning_rate=0.001,epoch=10,batch=64,activation=\"tanh\",beta=0.9,beta1=0.999,optimizer=\"rmsprop\",init_strat=\"xavier\",weight_decay=0,loss_type=\"cross_entropy\"):\n",
        "    self.num_hidden_layers=num_hidden_layers\n",
        "    self.num_neurons_in_hidden_layer=num_neurons_in_hidden_layer\n",
        "    self.learning_rate=learning_rate\n",
        "    self.epoch=epoch\n",
        "    self.batch=batch\n",
        "    self.pre_activation_layer={}\n",
        "    self.activation_layer={}\n",
        "    self.weights={}\n",
        "    self.bias={}\n",
        "    self.d_pre_activation_layer={}\n",
        "    self.d_activation_layer={}\n",
        "    self.d_weights={}\n",
        "    self.d_bias={}\n",
        "    self.beta=beta\n",
        "    self.beta1=beta1\n",
        "    self.optimizer=optimizer\n",
        "    self.init_strat=init_strat\n",
        "    self.weight_decay=weight_decay\n",
        "    self.loss_type=loss_type\n",
        "    self.activation_fn=activation\n",
        "    if(activation==\"sigmoid\"):self.activation_fn=NeuralNetwork.actobj.sigmoid\n",
        "    if(activation==\"tanh\"):self.activation_fn=NeuralNetwork.actobj.tanh\n",
        "    if(activation==\"relu\"):self.activation_fn=NeuralNetwork.actobj.relu\n",
        "    if(activation==\"identity\"):self.activation_fn=NeuralNetwork.actobj.identity\n",
        "    if(loss_type==\"cross_entropy\"): self.loss_fn=NeuralNetwork.actobj.cross_entropy\n",
        "    if(loss_type==\"mean_square_error\"): self.loss_fn=NeuralNetwork.actobj.mean_square_error\n",
        "\n",
        "  def weight_initialization(self):\n",
        "    self.neurons_list=[self.input_size]\n",
        "\n",
        "    for i in range(self.num_hidden_layers):\n",
        "      self.neurons_list.append(self.num_neurons_in_hidden_layer)\n",
        "    self.neurons_list.append(self.output_size)\n",
        "\n",
        "    for i in range(len(self.neurons_list)-1):\n",
        "      if(self.init_strat==\"random\"):\n",
        "        self.weights[i+1]=np.random.default_rng().uniform(-1,1,(self.neurons_list[i+1],self.neurons_list[i]))\n",
        "        self.bias[i+1]=np.random.default_rng().uniform(-1,1,(self.neurons_list[i+1],1))\n",
        "      elif(self.init_strat==\"xavier\"):\n",
        "        self.weights[i+1]=np.random.randn(self.neurons_list[i+1],self.neurons_list[i])*np.sqrt(2/(self.neurons_list[i+1]+self.neurons_list[i]))\n",
        "        self.bias[i+1]=np.random.randn(self.neurons_list[i+1],1)*np.sqrt(2/(self.neurons_list[i+1]+1))\n",
        "  \n",
        "  def forward_propogation(self,x):\n",
        "    self.activation_layer[0]=x\n",
        "    for i in range(1,self.num_hidden_layers+1):\n",
        "      self.pre_activation_layer[i]=self.bias[i]+np.matmul(self.weights[i],self.activation_layer[i-1])\n",
        "      self.activation_layer[i]=self.activation_fn(self.pre_activation_layer[i])\n",
        "    self.pre_activation_layer[self.num_hidden_layers+1]=self.bias[self.num_hidden_layers+1]+np.matmul(self.weights[self.num_hidden_layers+1],self.activation_layer[self.num_hidden_layers])\n",
        "    self.activation_layer[self.num_hidden_layers+1]=NeuralNetwork.actobj.softmax(self.pre_activation_layer[self.num_hidden_layers+1])    \n",
        "    return self.activation_layer[self.num_hidden_layers+1]\n",
        "\n",
        "  def back_propogation(self,true_output,output):\n",
        "    d_pre_activation_layer={}\n",
        "    d_weights={}\n",
        "    d_activation_layer={}\n",
        "    d_bias={}\n",
        "    if(self.loss_type==\"cross_entropy\"):\n",
        "      d_pre_activation_layer[self.num_hidden_layers+1]=-1*(true_output-output)\n",
        "    elif(self.loss_type==\"mean_square_error\"):\n",
        "      d_pre_activation_layer[self.num_hidden_layers+1]=-2*(true_output-output)*(output*(1-output))\n",
        "    for i in range(self.num_hidden_layers+1,0,-1):\n",
        "      d_weights[i]=np.outer(d_pre_activation_layer[i],self.activation_layer[i-1].T)\n",
        "      d_bias[i]=d_pre_activation_layer[i]\n",
        "      if(i==1):break\n",
        "      d_activation_layer[i-1]=np.matmul(self.weights[i].T,d_pre_activation_layer[i])\n",
        "      d_pre_activation_layer[i-1]=np.multiply(d_activation_layer[i-1],self.activation_fn(self.pre_activation_layer[i-1],True))#element multiplication\n",
        "\n",
        "    return d_weights,d_bias,self.loss_fn(true_output,output)\n",
        "    \n",
        "\n",
        "  def flush_gradients(self):\n",
        "    for i in range(len(self.neurons_list)-1):\n",
        "      self.d_weights[i+1]=np.zeros((self.neurons_list[i+1],self.neurons_list[i]))\n",
        "      self.d_bias[i+1]=np.zeros((self.neurons_list[i+1],1))\n",
        "\n",
        "  def accuracy(self,images,labels):\n",
        "    count=0\n",
        "    loss=0\n",
        "    for i in range(images.shape[0]):\n",
        "      output=self.forward_propogation(images[i].reshape((-1,1)))\n",
        "      true_output=np.zeros((self.output_size,1))\n",
        "      true_output[labels[i]]=1\n",
        "      if(labels[i]==np.argmax(output)):\n",
        "        count+=1\n",
        "      loss+=self.loss_fn(true_output,output)\n",
        "    \n",
        "    for i in range(1,self.num_hidden_layers+2):\n",
        "      loss+=(self.weight_decay/2)*np.sum(np.square(self.weights[i]))\n",
        "\n",
        "    return count*1.0/images.shape[0], loss/images.shape[0]\n",
        "    \n",
        "\n",
        "  def run(self,train_images,train_labels,validation_images,validation_labels):\n",
        "    self.input_size=train_images.shape[1]*train_images.shape[2]\n",
        "    self.output_size=10\n",
        "    self.weight_initialization()\n",
        "\n",
        "    if self.optimizer==\"sgd\":\n",
        "      opt=sgd()\n",
        "    elif self.optimizer==\"momentum\":\n",
        "      opt=momentum(self.neurons_list)\n",
        "    elif self.optimizer==\"rmsprop\":\n",
        "      opt=rmsprop(self.neurons_list)\n",
        "    elif self.optimizer==\"nag\":\n",
        "      opt=nag(self.neurons_list,self.weights,self.bias)\n",
        "    elif self.optimizer==\"adam\":\n",
        "      opt=adam(self.neurons_list)\n",
        "    elif self.optimizer==\"nadam\":\n",
        "      opt=nadam(self.neurons_list)\n",
        "\n",
        "    t=0\n",
        "    for epoch_no in range(self.epoch):\n",
        "      val_loss,tra_loss,tra_acc,val_acc=0,0,0,0\n",
        "      loss=0\n",
        "      batch_count=0\n",
        "      self.flush_gradients()\n",
        "      for i in range(train_images.shape[0]):\n",
        "        batch_count+=1\n",
        "        true_output=np.zeros((self.output_size,1))\n",
        "        true_output[train_labels[i]]=1\n",
        "        output=self.forward_propogation(train_images[i].reshape((-1,1)))\n",
        "        d_weights,d_bias,loss=self.back_propogation(true_output,output)\n",
        "        for j in range(1,self.num_hidden_layers+2):\n",
        "            self.d_weights[j]=self.d_weights[j]+d_weights[j]\n",
        "            self.d_bias[j]=self.d_bias[j]+d_bias[j]\n",
        "\n",
        "\n",
        "        if(batch_count==self.batch):\n",
        "          for j in range(1,self.num_hidden_layers+2):\n",
        "            self.d_weights[j]=self.d_weights[j]+self.weight_decay*self.weights[j]\n",
        "\n",
        "          self.d_weights = {k: v / self.batch for k, v in self.d_weights.items()}\n",
        "          self.d_bias = {k: v / self.batch for k, v in self.d_bias.items()}\n",
        "          batch_count=0\n",
        "          if self.optimizer==\"sgd\":\n",
        "            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias)\n",
        "          elif self.optimizer==\"momentum\":\n",
        "            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta)\n",
        "          elif self.optimizer==\"rmsprop\":\n",
        "            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta)\n",
        "          elif self.optimizer==\"adam\":\n",
        "            t+=1\n",
        "            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta,self.beta1,t)\n",
        "          elif self.optimizer==\"nadam\":\n",
        "            t+=1\n",
        "            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta,self.beta1,t)\n",
        "          elif self.optimizer==\"nag\":\n",
        "            self.weights,self.bias,self.weights_use,self.bias_use=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta)\n",
        "\n",
        "          self.flush_gradients()\n",
        "\n",
        "      if(batch_count>0):#remaining\n",
        "          for j in range(1,self.num_hidden_layers+2):\n",
        "            self.d_weights[j]=self.d_weights[j]+self.weight_decay*self.weights[j]\n",
        "\n",
        "          self.d_weights = {k: v / batch_count for k, v in self.d_weights.items()}\n",
        "          self.d_bias = {k: v / batch_count for k, v in self.d_bias.items()}\n",
        "          batch_count=0\n",
        "          if self.optimizer==\"sgd\":\n",
        "            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias)\n",
        "          elif self.optimizer==\"momentum\":\n",
        "            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta)\n",
        "          elif self.optimizer==\"rmsprop\":\n",
        "            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta)\n",
        "          elif self.optimizer==\"adam\":\n",
        "            t+=1\n",
        "            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta,self.beta1,t)\n",
        "          elif self.optimizer==\"nadam\":\n",
        "            t+=1\n",
        "            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta,self.beta1,t)\n",
        "          elif self.optimizer==\"nag\":\n",
        "            self.weights,self.bias,self.weights_use,self.bias_use=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta)\n",
        "\n",
        "          self.flush_gradients()     \n",
        "\n",
        "      val_acc,val_loss=self.accuracy(validation_images,validation_labels)\n",
        "      tra_acc,tra_loss=self.accuracy(train_images,train_labels) \n",
        "      print(epoch_no+1,'Training loss',tra_loss,'Val loss',val_loss,'Training Accuracy',tra_acc,'Val Accuracy',val_acc)\n",
        "      wandb.log({\"Training loss\":tra_loss,'Val loss':val_loss,'Training Accuracy':tra_acc,'Val Accuracy':val_acc})\n",
        "    if self.optimizer==\"nag\":\n",
        "      self.weights=self.weights_use #for nag\n",
        "      self.bias=self.bias_use #for nag\n"
      ],
      "metadata": {
        "id": "KJIzKltcZH96"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q7**"
      ],
      "metadata": {
        "id": "8P9qj7Urb96p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn=NeuralNetwork(3,64,0.001,10,64,\"tanh\",0.9,\"rmsprop\",\"xavier\",0,\"cross_entopy\")\n",
        "nn.run(train_images,train_labels,validation_images,validation_labels)\n",
        "pred=[]\n",
        "labels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "for i in range(len(test_images)):\n",
        "  pred.append(np.argmax(nn.forward_propogation(test_images[i].reshape((-1,1)))))\n",
        "\n",
        "wandb.log({\"confusion_matrix\" : wandb.plot.confusion_matrix(probs=None,y_true=test_labels, preds=pred,class_names=labels)})"
      ],
      "metadata": {
        "id": "0t347NWYaqu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q10**"
      ],
      "metadata": {
        "id": "Pnx0gPE4cLFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = keras.datasets.mnist\n",
        "(train_images_m, train_labels_m), (test_images_m, test_labels_m) = mnist.load_data()\n",
        "train_images_m = train_images_m / 255.0\n",
        "test_images_m = test_images_m / 255.0\n",
        "idx = np.arange(train_images_m.shape[0])\n",
        "np.random.shuffle(idx)\n",
        "train_images_m=train_images_m[idx]\n",
        "train_labels_m=train_labels_m[idx]\n",
        "validation_images_m=train_images_m[:6000]\n",
        "validation_labels_m=train_labels_m[:6000]\n",
        "train_images_m=train_images_m[6000:]\n",
        "train_labels_m=train_labels_m[6000:]\n",
        "\n",
        "nn1=NeuralNetwork(3,64,0.001,10,64,\"tanh\",0.9,0.999,\"rmsprop\",\"xavier\",0,\"cross_entropy\")\n",
        "nn1.run(train_images_m,train_labels_m,validation_images_m,validation_labels_m)\n",
        "\n",
        "nn2=NeuralNetwork(3,128,0.001,10,32,\"tanh\",0.9,\"rmsprop\",\"xavier\",0,\"cross_entropy\")\n",
        "nn2.run(train_images_m,train_labels_m,validation_images_m,validation_labels_m)\n",
        "\n",
        "nn3=NeuralNetwork(5,128,0.001,5,32,\"tanh\",0.9,\"rmsprop\",\"xavier\",0.0005,\"cross_entropy\")\n",
        "nn3.run(train_images_m,train_labels_m,validation_images_m,validation_labels_m)"
      ],
      "metadata": {
        "id": "t0t6eLO4cRmr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}