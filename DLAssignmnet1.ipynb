{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nO6NKbWijAQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXI5qaJYj47i"
      },
      "source": [
        "**Q1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iefc-kJ7iW4m",
        "outputId": "480c09ed-776d-4205-a5ef-58eb4476c2bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "validation_images=train_images[:6000]\n",
        "validation_labels=train_labels[:6000]\n",
        "train_images=train_images[6000:]\n",
        "train_labels=train_labels[6000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgSJQxUhkDtO"
      },
      "source": [
        "**Q2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dheY7mtjg4xT"
      },
      "outputs": [],
      "source": [
        "class ActivationFn:\n",
        "  def sigmoid(self,layer,d=False):\n",
        "      if(d==True):\n",
        "          return self.sigmoid(layer)*(1-self.sigmoid(layer))\n",
        "      return 1.0/(1.0+np.exp(-1.0*layer))\n",
        "\n",
        "  def softmax(self,layer):\n",
        "      #print(layer)\n",
        "      newlayer=(layer-np.max(layer))\n",
        "      #print(new_layer)\n",
        "      return np.exp(newlayer)/np.sum(np.exp(newlayer))\n",
        "\n",
        "  # def d_sigmoid(layer):\n",
        "  #     return sigmoid(layer)*(1-sigmoid(layer))\n",
        "\n",
        "  def cross_entropy(self,true_output,output):\n",
        "      return -1.0*np.sum(true_output * np.log(output+1e-9))\n",
        "\n",
        "  def tanh(self,layer,d=False):\n",
        "      if(d==True):\n",
        "          return 1-(np.exp(layer)-np.exp(-layer))/(np.exp(layer)+np.exp(-layer))**2\n",
        "      return (np.exp(layer)-np.exp(-layer))/(np.exp(layer)+np.exp(-layer))\n",
        "\n",
        "  # def d_tanh(layer):\n",
        "  #     return 1-(np.exp(layer)-np.exp(-layer))/(np.exp(layer)+np.exp(-layer))**2\n",
        "\n",
        "  def relu(self,layer,d=False):\n",
        "      if(d==True):\n",
        "        return 1. * (layer > 0)\n",
        "      return layer * (layer > 0)\n",
        "\n",
        "  # def d_relu(layer):\n",
        "  #     return 1. * (layer > 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "op3KfamMirc3"
      },
      "outputs": [],
      "source": [
        "input_size=28*28\n",
        "output_size=10\n",
        "num_hidden_layers=2\n",
        "num_neurons_in_hidden_layer=16\n",
        "pre_activation_layer={}\n",
        "activation_layer={}\n",
        "neurons_list=[]\n",
        "weights={}\n",
        "bias={}\n",
        "input_layer=np.random.rand(28*28,1)#to be defined n*1\n",
        "# output_layer=np.zeros((output_labels,1))\n",
        "\n",
        "neurons_list.append(input_size)\n",
        "for i in range(num_hidden_layers):\n",
        "  neurons_list.append(num_neurons_in_hidden_layer)\n",
        "neurons_list.append(output_size)\n",
        "\n",
        "#hidden_layer[0]=input_layer\n",
        "# for i in range(num_hidden_layers):\n",
        "#   hidden_layer.append(np.zeros(1,num_neurons_in_hidden_layer))\n",
        "# hidden_layer.append(output_layer)\n",
        "\n",
        "activation_layer[0]=input_layer\n",
        "# for i in range(num_hidden_layers):\n",
        "#   activation_layer.append(np.zeros(1,num_neurons_in_hidden_layer))\n",
        "# activation_layer.append(output_layer)\n",
        "\n",
        "for i in range(len(neurons_list)-1):\n",
        "  weights[i+1]=np.random.rand(neurons_list[i+1],neurons_list[i])\n",
        "\n",
        "for i in range(len(neurons_list)-1):\n",
        "  bias[i+1]=np.random.rand(neurons_list[i+1],1)\n",
        "\n",
        "\n",
        "def forward_propogation():\n",
        "  for i in range(1,num_hidden_layers+1):\n",
        "    pre_activation_layer[i]=bias[i]+np.matmul(weights[i],activation_layer[i-1])\n",
        "    activation_layer[i]=sigmoid(pre_activation_layer[i])\n",
        "  pre_activation_layer[num_hidden_layers+1]=bias[num_hidden_layers+1]+np.matmul(weights[num_hidden_layers+1],activation_layer[num_hidden_layers])\n",
        "  activation_layer[num_hidden_layers+1]=softmax(pre_activation_layer[num_hidden_layers+1])\n",
        "\n",
        "forward_propogation()\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmpWlYpZ_9F8"
      },
      "outputs": [],
      "source": [
        "class FeedFowardNeuralNetwork:\n",
        "  def __init__(self,num_hidden_layers,num_neurons_in_hidden_layer):\n",
        "    self.num_hidden_layers=num_hidden_layers\n",
        "    self.num_neurons_in_hidden_layer=num_neurons_in_hidden_layer\n",
        "    \n",
        "  def weight_initialization(self):\n",
        "    self.pre_activation_layer={}\n",
        "    self.activation_layer={}\n",
        "    self.weights={}\n",
        "    self.bias={}\n",
        "    self.neurons_list=[self.input_size]\n",
        "\n",
        "    for i in range(self.num_hidden_layers):\n",
        "      self.neurons_list.append(self.num_neurons_in_hidden_layer)\n",
        "    self.neurons_list.append(self.output_size)\n",
        "\n",
        "    self.activation_layer[0]=self.input_layer\n",
        "\n",
        "    for i in range(len(self.neurons_list)-1):\n",
        "      self.weights[i+1]=np.random.rand(self.neurons_list[i+1],self.neurons_list[i])\n",
        "\n",
        "    for i in range(len(self.neurons_list)-1):\n",
        "      self.bias[i+1]=np.random.rand(self.neurons_list[i+1],1)\n",
        "  \n",
        "  def forward_propogation(self):\n",
        "    for i in range(1,self.num_hidden_layers+1):\n",
        "      self.pre_activation_layer[i]=self.bias[i]+np.matmul(self.weights[i],self.activation_layer[i-1])\n",
        "      self.activation_layer[i]=sigmoid(self.pre_activation_layer[i])\n",
        "    self.pre_activation_layer[self.num_hidden_layers+1]=self.bias[self.num_hidden_layers+1]+np.matmul(self.weights[self.num_hidden_layers+1],self.activation_layer[self.num_hidden_layers])\n",
        "    self.activation_layer[self.num_hidden_layers+1]=softmax(self.pre_activation_layer[self.num_hidden_layers+1])    \n",
        "    return self.activation_layer[self.num_hidden_layers+1]\n",
        "\n",
        "  def run(self,X,Y):\n",
        "    self.input_layer=X\n",
        "    self.input_size=len(X)\n",
        "    self.output_size=Y\n",
        "    self.weight_initialization()\n",
        "    return self.forward_propogation()\n",
        "\n",
        "\n",
        "nn=FeedFowardNeuralNetwork(1,128)\n",
        "print(nn.run(train_images[0].reshape((-1,1)),10))# Converting to column vector\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BackpropogationNN:\n",
        "  def __init__(self,num_hidden_layers,num_neurons_in_hidden_layer):\n",
        "    self.num_hidden_layers=num_hidden_layers\n",
        "    self.num_neurons_in_hidden_layer=num_neurons_in_hidden_layer\n",
        "    self.learning_rate=0.1\n",
        "    self.epoch=15\n",
        "    self.batch=32\n",
        "    # self.activation_f=act\n",
        "    self.pre_activation_layer={}\n",
        "    self.activation_layer={}\n",
        "    self.weights={}\n",
        "    self.bias={}\n",
        "    self.d_pre_activation_layer={}\n",
        "    self.d_activation_layer={}\n",
        "    self.d_weights={}\n",
        "    self.d_bias={}\n",
        "\n",
        "  def weight_initialization(self):\n",
        "    self.neurons_list=[self.input_size]\n",
        "\n",
        "    for i in range(self.num_hidden_layers):\n",
        "      self.neurons_list.append(self.num_neurons_in_hidden_layer)\n",
        "    self.neurons_list.append(self.output_size)\n",
        "\n",
        "    #self.pre_activation_layer[0]=self.input_layer #to be checked\n",
        "\n",
        "    for i in range(len(self.neurons_list)-1):\n",
        "      self.weights[i+1]=np.random.uniform(-1,1,(self.neurons_list[i+1],self.neurons_list[i]))\n",
        "\n",
        "    for i in range(len(self.neurons_list)-1):\n",
        "      self.bias[i+1]=np.random.uniform(-1,1,(self.neurons_list[i+1],1))\n",
        "  \n",
        "  def forward_propogation(self,x):\n",
        "    self.activation_layer[0]=x\n",
        "    for i in range(1,self.num_hidden_layers+1):\n",
        "      self.pre_activation_layer[i]=self.bias[i]+np.matmul(self.weights[i],self.activation_layer[i-1])\n",
        "      self.activation_layer[i]=sigmoid(self.pre_activation_layer[i])\n",
        "    self.pre_activation_layer[self.num_hidden_layers+1]=self.bias[self.num_hidden_layers+1]+np.matmul(self.weights[self.num_hidden_layers+1],self.activation_layer[self.num_hidden_layers])\n",
        "    self.activation_layer[self.num_hidden_layers+1]=softmax(self.pre_activation_layer[self.num_hidden_layers+1])    \n",
        "    return self.activation_layer[self.num_hidden_layers+1]\n",
        "\n",
        "  def back_propogation(self,true_output,output):\n",
        "    d_pre_activation_layer={}\n",
        "    d_weights={}\n",
        "    d_activation_layer={}\n",
        "    d_bias={}\n",
        "    d_pre_activation_layer[self.num_hidden_layers+1]=-1*(true_output-output)\n",
        "    for i in range(self.num_hidden_layers+1,0,-1):\n",
        "      d_weights[i]=np.outer(d_pre_activation_layer[i],self.activation_layer[i-1].transpose())\n",
        "      d_bias[i]=d_pre_activation_layer[i]\n",
        "      if(i==1):break\n",
        "      d_activation_layer[i-1]=np.matmul(self.weights[i].transpose(),d_pre_activation_layer[i])\n",
        "      d_pre_activation_layer[i-1]=np.multiply(d_activation_layer[i-1],d_sigmoid(self.pre_activation_layer[i-1]))#element multiplication\n",
        "      #condn for index 0\n",
        "    return d_weights,d_bias,cross_entropy(true_output,output)\n",
        "    \n",
        "  def update_parameters(self):\n",
        "    self.d_weights = {k: v / self.batch for k, v in self.d_weights.items()}\n",
        "    self.d_bias = {k: v / self.batch for k, v in self.d_bias.items()}\n",
        "    for i in range(1,self.num_hidden_layers+2):\n",
        "      self.weights[i]=self.weights[i]-self.learning_rate*self.d_weights[i]\n",
        "    for i in range(1,self.num_hidden_layers+2):\n",
        "      self.bias[i]=self.bias[i]-self.learning_rate*self.d_bias[i]\n",
        "\n",
        "  def flush_gradients(self):\n",
        "    self.d_weights={}\n",
        "    self.d_bias={}\n",
        "\n",
        "  def run(self,train_images,train_labels):\n",
        "    self.input_size=train_images.shape[1]*train_images.shape[2]\n",
        "    self.output_size=10\n",
        "    self.weight_initialization()\n",
        "    for epoch_no in range(self.epoch):\n",
        "      loss=0\n",
        "      batch_count=0\n",
        "      for i in range(train_images.shape[0]):\n",
        "        batch_count+=1\n",
        "        #self.input_layer=train_images[i].reshape((-1,1))\n",
        "        true_output=np.zeros((self.output_size,1))\n",
        "        true_output[train_labels[i]-1]=1\n",
        "\n",
        "        output=self.forward_propogation(train_images[i].reshape((-1,1)))\n",
        "        #print(np.argmax(output))\n",
        "        d_weights,d_bias,cur_loss=self.back_propogation(true_output,output)\n",
        "        # if(epoch_no==1):\n",
        "        #    print(d_weights)\n",
        "        loss+=cur_loss\n",
        "        for j in range(1,self.num_hidden_layers+2):\n",
        "          if self.d_weights.get(j) is not None:\n",
        "            self.d_weights[j]=self.d_weights[j]+d_weights[j]\n",
        "          else:\n",
        "            self.d_weights[j]=d_weights[j]\n",
        "          if self.d_bias.get(j) is not None:\n",
        "            self.d_bias[j]=self.d_bias[j]+d_bias[j]\n",
        "          else:\n",
        "            self.d_bias[j]=d_bias[j]\n",
        "        \n",
        "        if(batch_count==self.batch):\n",
        "          batch_count=0\n",
        "          self.update_parameters()\n",
        "          self.flush_gradients()\n",
        "\n",
        "      if(batch_count>0):#remaining\n",
        "          batch_count=0\n",
        "          self.update_parameters()\n",
        "          self.flush_gradients()        \n",
        "\n",
        "    \n",
        "      print(epoch_no+1,loss/train_images.shape[0])\n",
        "      #self.d_weights = {k: v / train_images.shape[0] for k, v in self.d_weights.items()}\n",
        "      #self.d_bias = {k: v / train_images.shape[0] for k, v in self.d_bias.items()}\n",
        "      # self.update_parameters()\n",
        "      self.flush_gradients()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "nn=BackpropogationNN(3,128)\n",
        "nn.run(train_images,train_labels)# Converting to column vector\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BackpropogationNN:\n",
        "  actobj=ActivationFn()\n",
        "  def __init__(self,num_hidden_layers,num_neurons_in_hidden_layer,learning_rate,epoch,batch,activation,beta,optimizer,init_strat):\n",
        "    self.num_hidden_layers=num_hidden_layers\n",
        "    self.num_neurons_in_hidden_layer=num_neurons_in_hidden_layer\n",
        "    self.learning_rate=learning_rate\n",
        "    self.epoch=epoch\n",
        "    self.batch=batch\n",
        "    self.pre_activation_layer={}\n",
        "    self.activation_layer={}\n",
        "    self.weights={}\n",
        "    self.bias={}\n",
        "    self.d_pre_activation_layer={}\n",
        "    self.d_activation_layer={}\n",
        "    self.d_weights={}\n",
        "    self.d_bias={}\n",
        "    self.beta=beta\n",
        "    self.beta1=0.999\n",
        "    self.optimizer=optimizer\n",
        "    self.init_strat=init_strat\n",
        "    # self.activation_fn=activation\n",
        "    if(activation==\"sigmoid\"):self.activation_fn=BackpropogationNN.actobj.sigmoid\n",
        "    if(activation==\"tanh\"):self.activation_fn=BackpropogationNN.actobj.tanh\n",
        "    if(activation==\"relu\"):self.activation_fn=BackpropogationNN.actobj.relu\n",
        "\n",
        "  def weight_initialization(self):\n",
        "    self.neurons_list=[self.input_size]\n",
        "\n",
        "    for i in range(self.num_hidden_layers):\n",
        "      self.neurons_list.append(self.num_neurons_in_hidden_layer)\n",
        "    self.neurons_list.append(self.output_size)\n",
        "\n",
        "    for i in range(len(self.neurons_list)-1):\n",
        "      if(self.init_strat==\"random\"):\n",
        "        self.weights[i+1]=np.random.default_rng().uniform(-1,1,(self.neurons_list[i+1],self.neurons_list[i]))\n",
        "        self.bias[i+1]=np.random.default_rng().uniform(-1,1,(self.neurons_list[i+1],1))\n",
        "      elif(self.init_strat==\"xavier\"):\n",
        "        self.weights[i+1]=np.random.randn(self.neurons_list[i+1],self.neurons_list[i])*np.sqrt(2/(self.neurons_list[i+1]+self.neurons_list[i]))\n",
        "        self.bias[i+1]=np.random.randn(self.neurons_list[i+1],1)*np.sqrt(2/(self.neurons_list[i+1]+1))\n",
        "  \n",
        "  def forward_propogation(self,x):\n",
        "    self.activation_layer[0]=x\n",
        "    for i in range(1,self.num_hidden_layers+1):\n",
        "      self.pre_activation_layer[i]=self.bias[i]+np.matmul(self.weights[i],self.activation_layer[i-1])\n",
        "      self.activation_layer[i]=self.activation_fn(self.pre_activation_layer[i])\n",
        "    self.pre_activation_layer[self.num_hidden_layers+1]=self.bias[self.num_hidden_layers+1]+np.matmul(self.weights[self.num_hidden_layers+1],self.activation_layer[self.num_hidden_layers])\n",
        "    self.activation_layer[self.num_hidden_layers+1]=BackpropogationNN.actobj.softmax(self.pre_activation_layer[self.num_hidden_layers+1])    \n",
        "    return self.activation_layer[self.num_hidden_layers+1]\n",
        "\n",
        "  def back_propogation(self,true_output,output):\n",
        "    d_pre_activation_layer={}\n",
        "    d_weights={}\n",
        "    d_activation_layer={}\n",
        "    d_bias={}\n",
        "    d_pre_activation_layer[self.num_hidden_layers+1]=-1*(true_output-output)\n",
        "    for i in range(self.num_hidden_layers+1,0,-1):\n",
        "      d_weights[i]=np.outer(d_pre_activation_layer[i],self.activation_layer[i-1].T)\n",
        "      d_bias[i]=d_pre_activation_layer[i]\n",
        "      if(i==1):break\n",
        "      d_activation_layer[i-1]=np.matmul(self.weights[i].T,d_pre_activation_layer[i])\n",
        "      d_pre_activation_layer[i-1]=np.multiply(d_activation_layer[i-1],self.activation_fn(self.pre_activation_layer[i-1],True))#element multiplication\n",
        "\n",
        "    return d_weights,d_bias,BackpropogationNN.actobj.cross_entropy(true_output,output)\n",
        "    \n",
        "\n",
        "  def flush_gradients(self):\n",
        "    for i in range(len(self.neurons_list)-1):\n",
        "      self.d_weights[i+1]=np.zeros((self.neurons_list[i+1],self.neurons_list[i]))\n",
        "      self.d_bias[i+1]=np.zeros((self.neurons_list[i+1],1))\n",
        "\n",
        "  def accuracy(self,images,labels):\n",
        "    count=0\n",
        "    loss=0\n",
        "    for i in range(images.shape[0]):\n",
        "      output=self.forward_propogation(images[i].reshape((-1,1)))\n",
        "      true_output=np.zeros((self.output_size,1))\n",
        "      true_output[labels[i]]=1\n",
        "      if(labels[i]==np.argmax(output)):\n",
        "        count+=1\n",
        "      loss+=BackpropogationNN.actobj.cross_entropy(true_output,output)\n",
        "\n",
        "    return count*1.0/images.shape[0], loss/images.shape[0]\n",
        "    \n",
        "\n",
        "  def run(self,train_images,train_labels,validation_images,validation_labels):\n",
        "    self.input_size=train_images.shape[1]*train_images.shape[2]\n",
        "    self.output_size=10\n",
        "    self.weight_initialization()\n",
        "\n",
        "    if self.optimizer==\"sgd\":\n",
        "      opt=sgd()\n",
        "    elif self.optimizer==\"momentum\":\n",
        "      opt=momentum(self.neurons_list)\n",
        "    elif self.optimizer==\"rmsprop\":\n",
        "      opt=rmsprop(self.neurons_list)\n",
        "    elif self.optimizer==\"nag\":\n",
        "      opt=nag(self.neurons_list,self.weights,self.bias)\n",
        "    elif self.optimizer==\"adam\":\n",
        "      opt=adam(self.neurons_list)\n",
        "    elif self.optimizer==\"nadam\":\n",
        "      opt=nadam(self.neurons_list)\n",
        "\n",
        "    t=0\n",
        "    for epoch_no in range(self.epoch):\n",
        "      val_loss,tra_loss,tra_acc,val_acc=0,0,0,0\n",
        "      loss=0\n",
        "      batch_count=0\n",
        "      self.flush_gradients()\n",
        "      for i in range(train_images.shape[0]):\n",
        "        batch_count+=1\n",
        "        true_output=np.zeros((self.output_size,1))\n",
        "        true_output[train_labels[i]]=1\n",
        "        output=self.forward_propogation(train_images[i].reshape((-1,1)))\n",
        "        d_weights,d_bias,cur_loss=self.back_propogation(true_output,output)\n",
        "        loss+=cur_loss\n",
        "        for j in range(1,self.num_hidden_layers+2):\n",
        "            self.d_weights[j]=self.d_weights[j]+d_weights[j]\n",
        "            self.d_bias[j]=self.d_bias[j]+d_bias[j]\n",
        "\n",
        "\n",
        "        if(batch_count==self.batch):\n",
        "          self.d_weights = {k: v / self.batch for k, v in self.d_weights.items()}\n",
        "          self.d_bias = {k: v / self.batch for k, v in self.d_bias.items()}\n",
        "          batch_count=0\n",
        "          if self.optimizer==\"sgd\":\n",
        "            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias)\n",
        "          elif self.optimizer==\"momentum\":\n",
        "            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta)\n",
        "          elif self.optimizer==\"rmsprop\":\n",
        "            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta)\n",
        "          elif self.optimizer==\"adam\":\n",
        "            t+=1\n",
        "            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta,self.beta1,t)\n",
        "          elif self.optimizer==\"nadam\":\n",
        "            t+=1\n",
        "            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta,self.beta1,t)\n",
        "          elif self.optimizer==\"nag\":\n",
        "            self.weights,self.bias,self.weights_use,self.bias_use=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta)\n",
        "\n",
        "          self.flush_gradients()\n",
        "\n",
        "      if(batch_count>0):#remaining\n",
        "          self.d_weights = {k: v / batch_count for k, v in self.d_weights.items()}\n",
        "          self.d_bias = {k: v / batch_count for k, v in self.d_bias.items()}\n",
        "          batch_count=0\n",
        "          if self.optimizer==\"sgd\":\n",
        "            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias)\n",
        "          elif self.optimizer==\"momentum\":\n",
        "            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta)\n",
        "          elif self.optimizer==\"rmsprop\":\n",
        "            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta)\n",
        "          elif self.optimizer==\"adam\":\n",
        "            t+=1\n",
        "            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta,self.beta1,t)\n",
        "          elif self.optimizer==\"nadam\":\n",
        "            t+=1\n",
        "            self.weights,self.bias=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta,self.beta1,t)\n",
        "          elif self.optimizer==\"nag\":\n",
        "            self.weights,self.bias,self.weights_use,self.bias_use=opt.optimize(self.num_hidden_layers,self.weights,self.bias,self.learning_rate,self.d_weights,self.d_bias,self.beta)\n",
        "\n",
        "          self.flush_gradients()     \n",
        "\n",
        "      val_acc,val_loss=self.accuracy(validation_images,validation_labels)\n",
        "      tra_acc,tra_loss=self.accuracy(train_images,train_labels) \n",
        "      print(epoch_no+1,'Training loss',tra_loss,'Val loss',val_loss,'Training Accuracy',tra_acc,'Val Accuracy',val_acc)\n",
        "      #wandb.log({\"Training loss\":tra_loss,'Val loss':val_loss,'Training Accuracy':tra_acc,'Val Accuracy':val_acc})\n",
        "    if self.optimizer==\"nag\":\n",
        "      self.weights=self.weights_use #for nag\n",
        "      self.bias=self.bias_use #for nag\n",
        "\n",
        "\n",
        "#num_hidden_layers,num_neurons_in_hidden_layer,learning_rate,epoch,batch,activation,beta,optimizer\n",
        "nn=BackpropogationNN(3,128,0.0001,10,32,\"relu\",0.9,\"adam\",\"xavier\")\n",
        "nn.run(train_images,train_labels,validation_images,validation_labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class sgd:\n",
        "  def optimize(self,num_hidden_layers,weights,bias,learning_rate,d_weights,d_bias):\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "      weights[i]=weights[i]-learning_rate*d_weights[i]\n",
        "      bias[i]=bias[i]-learning_rate*d_bias[i]\n",
        "    return weights,bias\n",
        "\n",
        "class momentum:\n",
        "  def __init__(self,neurons_list):\n",
        "    self.update_w={}\n",
        "    self.update_b={}\n",
        "    for i in range(len(neurons_list)-1):\n",
        "      self.update_w[i+1]=np.zeros((neurons_list[i+1],neurons_list[i]))\n",
        "      self.update_b[i+1]=np.zeros((neurons_list[i+1],1))\n",
        "  def optimize(self,num_hidden_layers,weights,bias,learning_rate,d_weights,d_bias,beta):\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "        self.update_w[i]=beta*self.update_w[i]+d_weights[i]\n",
        "        self.update_b[i]=beta*self.update_b[i]+d_bias[i]\n",
        "\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "      weights[i]=weights[i]-learning_rate*self.update_w[i]\n",
        "      bias[i]=bias[i]-learning_rate*self.update_b[i]\n",
        "\n",
        "    return weights,bias\n",
        "\n",
        "class nag:\n",
        "  def __init__(self,neurons_list,weights,bias):\n",
        "    self.update_w={}\n",
        "    self.update_b={}\n",
        "    self.weights_use={}\n",
        "    self.bias_use={}\n",
        "    for i in range(len(neurons_list)-1):\n",
        "      self.update_w[i+1]=np.zeros((neurons_list[i+1],neurons_list[i]))\n",
        "      self.update_b[i+1]=np.zeros((neurons_list[i+1],1))\n",
        "    self.bias_use=bias.copy()\n",
        "    self.weights_use=weights.copy()\n",
        "  \n",
        "  def optimize(self,num_hidden_layers,weights,bias,learning_rate,d_weights,d_bias,beta):\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "        self.update_w[i]=beta*self.update_w[i]+d_weights[i]\n",
        "        self.update_b[i]=beta*self.update_b[i]+d_bias[i]\n",
        "\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "      self.weights_use[i]=self.weights_use[i]-learning_rate*self.update_w[i]\n",
        "      self.bias_use[i]=self.bias_use[i]-learning_rate*self.update_b[i]\n",
        "\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "      weights[i]=self.weights_use[i]-beta*self.update_w[i]\n",
        "      bias[i]=self.bias_use[i]-beta*self.update_b[i] \n",
        "\n",
        "    return weights,bias,self.weights_use,self.bias_use\n",
        "\n",
        "class rmsprop:\n",
        "  def __init__(self,neurons_list):\n",
        "    self.update_w={}\n",
        "    self.update_b={}\n",
        "    self.epsilon=1e-6\n",
        "    for i in range(len(neurons_list)-1):\n",
        "      self.update_w[i+1]=np.zeros((neurons_list[i+1],neurons_list[i]))\n",
        "      self.update_b[i+1]=np.zeros((neurons_list[i+1],1))\n",
        "\n",
        "  def optimize(self,num_hidden_layers,weights,bias,learning_rate,d_weights,d_bias,beta):\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "        self.update_w[i]=beta*self.update_w[i]+(1-beta)*((d_weights[i])**2)\n",
        "        self.update_b[i]=beta*self.update_b[i]+(1-beta)*((d_bias[i])**2)\n",
        "\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "      weights[i]=weights[i]-learning_rate*d_weights[i]/(np.sqrt(self.update_w[i])+self.epsilon)\n",
        "      bias[i]=bias[i]-learning_rate*d_bias[i]/(np.sqrt(self.update_b[i])+self.epsilon)\n",
        "      \n",
        "    return weights,bias\n",
        "\n",
        "\n",
        "\n",
        "class adam:\n",
        "  def __init__(self,neurons_list):\n",
        "    self.update_w={}\n",
        "    self.update_b={}\n",
        "    self.update_what={}\n",
        "    self.update_bhat={}\n",
        "    self.momentum_w={}\n",
        "    self.momentum_b={}\n",
        "    self.momentum_what={}\n",
        "    self.momentum_bhat={}\n",
        "    self.epsilon=1e-6\n",
        "    for i in range(len(neurons_list)-1):\n",
        "      self.update_w[i+1]=np.zeros((neurons_list[i+1],neurons_list[i]))\n",
        "      self.update_b[i+1]=np.zeros((neurons_list[i+1],1))\n",
        "      self.momentum_w[i+1]=np.zeros((neurons_list[i+1],neurons_list[i]))\n",
        "      self.momentum_b[i+1]=np.zeros((neurons_list[i+1],1))\n",
        "  def optimize(self,num_hidden_layers,weights,bias,learning_rate,d_weights,d_bias,beta1,beta2,t):\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "        self.momentum_w[i]=beta1*self.momentum_w[i]+(1-beta1)*((d_weights[i]))\n",
        "        self.momentum_what[i]=self.momentum_w[i]/(1-beta1**t)\n",
        "\n",
        "        self.momentum_b[i]=beta1*self.momentum_b[i]+(1-beta1)*((d_bias[i]))\n",
        "        self.momentum_bhat[i]=self.momentum_b[i]/(1-beta1**t)\n",
        "\n",
        "        self.update_w[i]=beta2*self.update_w[i]+(1-beta2)*((d_weights[i])**2)\n",
        "        self.update_what[i]=self.update_w[i]/(1-beta2**t)\n",
        "\n",
        "        self.update_b[i]=beta2*self.update_b[i]+(1-beta2)*((d_bias[i])**2)\n",
        "        self.update_bhat[i]=self.update_b[i]/(1-beta2**t)\n",
        "  \n",
        "\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "      weights[i]=weights[i]-learning_rate*self.momentum_what[i]/(np.sqrt(self.update_what[i])+self.epsilon)\n",
        "      bias[i]=bias[i]-learning_rate*self.momentum_bhat[i]/(np.sqrt(self.update_bhat[i])+self.epsilon)\n",
        "      \n",
        "    return weights,bias\n",
        "    \n",
        "    \n",
        "\n",
        "class nadam:\n",
        "  def __init__(self,neurons_list):\n",
        "    self.update_w={}\n",
        "    self.update_b={}\n",
        "    self.update_what={}\n",
        "    self.update_bhat={}\n",
        "    self.momentum_w={}\n",
        "    self.momentum_b={}\n",
        "    self.momentum_what={}\n",
        "    self.momentum_bhat={}\n",
        "    self.epsilon=1e-6\n",
        "    for i in range(len(neurons_list)-1):\n",
        "      self.update_w[i+1]=np.zeros((neurons_list[i+1],neurons_list[i]))\n",
        "      self.update_b[i+1]=np.zeros((neurons_list[i+1],1))\n",
        "      self.momentum_w[i+1]=np.zeros((neurons_list[i+1],neurons_list[i]))\n",
        "      self.momentum_b[i+1]=np.zeros((neurons_list[i+1],1))\n",
        "  def optimize(self,num_hidden_layers,weights,bias,learning_rate,d_weights,d_bias,beta1,beta2,t):\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "        self.momentum_w[i]=beta1*self.momentum_w[i]+(1-beta1)*((d_weights[i]))\n",
        "        self.momentum_what[i]=self.momentum_w[i]/(1-beta1**t)\n",
        "\n",
        "        self.momentum_b[i]=beta1*self.momentum_b[i]+(1-beta1)*((d_bias[i]))\n",
        "        self.momentum_bhat[i]=self.momentum_b[i]/(1-beta1**t)\n",
        "\n",
        "        self.update_w[i]=beta2*self.update_w[i]+(1-beta2)*((d_weights[i])**2)\n",
        "        self.update_what[i]=self.update_w[i]/(1-beta2**t)\n",
        "\n",
        "        self.update_b[i]=beta2*self.update_b[i]+(1-beta2)*((d_bias[i])**2)\n",
        "        self.update_bhat[i]=self.update_b[i]/(1-beta2**t)\n",
        "  \n",
        "\n",
        "    for i in range(1,num_hidden_layers+2):\n",
        "      weights[i]=weights[i]-learning_rate/(np.sqrt(self.update_what[i])+self.epsilon)*(beta1*self.momentum_what[i]+(1-beta1)*d_weights[i]/(1-beta1**t))\n",
        "      bias[i]=bias[i]-learning_rate/(np.sqrt(self.update_bhat[i])+self.epsilon)*(beta1*self.momentum_bhat[i]+(1-beta1)*d_bias[i]/(1-beta1**t))\n",
        "      \n",
        "    return weights,bias\n",
        "  \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
