{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nO6NKbWijAQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXI5qaJYj47i"
      },
      "source": [
        "**Q1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iefc-kJ7iW4m",
        "outputId": "480c09ed-776d-4205-a5ef-58eb4476c2bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s64Az0JiFGjV"
      },
      "outputs": [],
      "source": [
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkkpXJHIDl_Q",
        "outputId": "8698ebad-41f9-44f3-9c32-7d9f4229f71d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_images.shape[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgSJQxUhkDtO"
      },
      "source": [
        "**Q2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dheY7mtjg4xT"
      },
      "outputs": [],
      "source": [
        "def sigmoid(layer,d=False):\n",
        "    if(d==True):\n",
        "        return sigmoid(layer)*(1-sigmoid(layer))\n",
        "    return 1.0/(1.0+np.exp(-1.0*layer))\n",
        "\n",
        "def softmax(layer):\n",
        "    #new_layer=(layer-np.min(layer))/(np.max(layer)-np.min(layer))\n",
        "    return np.exp(layer)/np.sum(np.exp(layer))\n",
        "\n",
        "def d_sigmoid(layer):\n",
        "    return sigmoid(layer)*(1-sigmoid(layer))\n",
        "\n",
        "def cross_entropy(true_output,output):\n",
        "    return -1.0*np.sum(true_output * np.log(output))\n",
        "\n",
        "def tanh(layer,d=False):\n",
        "    if(d==True):\n",
        "        return 1-(np.exp(layer)-np.exp(-layer))/(np.exp(layer)+np.exp(-layer))**2\n",
        "    return (np.exp(layer)-np.exp(-layer))/(np.exp(layer)+np.exp(-layer))\n",
        "\n",
        "def d_tanh(layer):\n",
        "    return 1-(np.exp(layer)-np.exp(-layer))/(np.exp(layer)+np.exp(-layer))**2\n",
        "\n",
        "def relu(layer,d=False):\n",
        "    if(d==True):\n",
        "      return 1. * (layer > 0)\n",
        "    return layer * (layer > 0)\n",
        "\n",
        "def d_relu(layer):\n",
        "    return 1. * (layer > 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "op3KfamMirc3"
      },
      "outputs": [],
      "source": [
        "input_size=28*28\n",
        "output_size=10\n",
        "num_hidden_layers=2\n",
        "num_neurons_in_hidden_layer=16\n",
        "pre_activation_layer={}\n",
        "activation_layer={}\n",
        "neurons_list=[]\n",
        "weights={}\n",
        "bias={}\n",
        "input_layer=np.random.rand(28*28,1)#to be defined n*1\n",
        "# output_layer=np.zeros((output_labels,1))\n",
        "\n",
        "neurons_list.append(input_size)\n",
        "for i in range(num_hidden_layers):\n",
        "  neurons_list.append(num_neurons_in_hidden_layer)\n",
        "neurons_list.append(output_size)\n",
        "\n",
        "#hidden_layer[0]=input_layer\n",
        "# for i in range(num_hidden_layers):\n",
        "#   hidden_layer.append(np.zeros(1,num_neurons_in_hidden_layer))\n",
        "# hidden_layer.append(output_layer)\n",
        "\n",
        "activation_layer[0]=input_layer\n",
        "# for i in range(num_hidden_layers):\n",
        "#   activation_layer.append(np.zeros(1,num_neurons_in_hidden_layer))\n",
        "# activation_layer.append(output_layer)\n",
        "\n",
        "for i in range(len(neurons_list)-1):\n",
        "  weights[i+1]=np.random.rand(neurons_list[i+1],neurons_list[i])\n",
        "\n",
        "for i in range(len(neurons_list)-1):\n",
        "  bias[i+1]=np.random.rand(neurons_list[i+1],1)\n",
        "\n",
        "\n",
        "def forward_propogation():\n",
        "  for i in range(1,num_hidden_layers+1):\n",
        "    pre_activation_layer[i]=bias[i]+np.matmul(weights[i],activation_layer[i-1])\n",
        "    activation_layer[i]=sigmoid(pre_activation_layer[i])\n",
        "  pre_activation_layer[num_hidden_layers+1]=bias[num_hidden_layers+1]+np.matmul(weights[num_hidden_layers+1],activation_layer[num_hidden_layers])\n",
        "  activation_layer[num_hidden_layers+1]=softmax(pre_activation_layer[num_hidden_layers+1])\n",
        "\n",
        "forward_propogation()\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmpWlYpZ_9F8"
      },
      "outputs": [],
      "source": [
        "class FeedFowardNeuralNetwork:\n",
        "  def __init__(self,num_hidden_layers,num_neurons_in_hidden_layer):\n",
        "    self.num_hidden_layers=num_hidden_layers\n",
        "    self.num_neurons_in_hidden_layer=num_neurons_in_hidden_layer\n",
        "    \n",
        "  def weight_initialization(self):\n",
        "    self.pre_activation_layer={}\n",
        "    self.activation_layer={}\n",
        "    self.weights={}\n",
        "    self.bias={}\n",
        "    self.neurons_list=[self.input_size]\n",
        "\n",
        "    for i in range(self.num_hidden_layers):\n",
        "      self.neurons_list.append(self.num_neurons_in_hidden_layer)\n",
        "    self.neurons_list.append(self.output_size)\n",
        "\n",
        "    self.activation_layer[0]=self.input_layer\n",
        "\n",
        "    for i in range(len(self.neurons_list)-1):\n",
        "      self.weights[i+1]=np.random.rand(self.neurons_list[i+1],self.neurons_list[i])\n",
        "\n",
        "    for i in range(len(self.neurons_list)-1):\n",
        "      self.bias[i+1]=np.random.rand(self.neurons_list[i+1],1)\n",
        "  \n",
        "  def forward_propogation(self):\n",
        "    for i in range(1,self.num_hidden_layers+1):\n",
        "      self.pre_activation_layer[i]=self.bias[i]+np.matmul(self.weights[i],self.activation_layer[i-1])\n",
        "      self.activation_layer[i]=sigmoid(self.pre_activation_layer[i])\n",
        "    self.pre_activation_layer[self.num_hidden_layers+1]=self.bias[self.num_hidden_layers+1]+np.matmul(self.weights[self.num_hidden_layers+1],self.activation_layer[self.num_hidden_layers])\n",
        "    self.activation_layer[self.num_hidden_layers+1]=softmax(self.pre_activation_layer[self.num_hidden_layers+1])    \n",
        "    return self.activation_layer[self.num_hidden_layers+1]\n",
        "\n",
        "  def run(self,X,Y):\n",
        "    self.input_layer=X\n",
        "    self.input_size=len(X)\n",
        "    self.output_size=Y\n",
        "    self.weight_initialization()\n",
        "    return self.forward_propogation()\n",
        "\n",
        "\n",
        "nn=FeedFowardNeuralNetwork(1,128)\n",
        "print(nn.run(train_images[0].reshape((-1,1)),10))# Converting to column vector\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
